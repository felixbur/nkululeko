@article{scikit-learn:2011,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{nkululeko:2022,
   author = {Felix Burkhardt and Johannes Wagner and Hagen Wierstorf and Florian Eyben and Björn Schuller},
   isbn = {9791095546726},
   journal = {2022 Language Resources and Evaluation Conference, LREC 2022},
   pages = {1925-1932},
   publisher = {European Language Resources Association (ELRA)},
   title = {Nkululeko: A Tool For Rapid Speaker Characteristics Detection},
   year = {2022},
}

@article{McFee:2015,
author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
doi = {10.25080/majora-7b98e3ed-003},
journal = {Proc. 14th Python Sci. Conf.},
number = {Scipy},
pages = {18--24},
title = {{librosa: Audio and Music Signal Analysis in Python}},
year = {2015},
}

@book{torch:2020,
   author = {Anmol Chaudhary and Kuldeep Singh Chouhan and Jyoti Gajrani and Bhavna Sharma},
   doi = {10.4018/978-1-7998-3095-5.ch003},
   title = {Deep Learning With PyTorch},
   year = {2020},
}

@inproceedings{Yang:2021,
author = {Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y. and Liu, Andy T. and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and Huang, Tzu-Hsien and Tseng, Wei-Cheng and Lee, Ko-tik and Liu, Da-Rong and Huang, Zili and Dong, Shuyan and Li, Shang-Wen and Watanabe, Shinji and Mohamed, Abdelrahman and Lee, Hung-yi},
booktitle = {Interspeech 2021},
doi = {10.21437/Interspeech.2021-1775},
eprint = {2105.01051},
keywords = {Benchmark,Evaluation,Model Generalization,Representation Learning,Self-Supervised Learning,Speech},
mendeley-groups = {jtes,superb,sent{\_}ser,four-not-six,exvo,multiser},
month = {aug},
pages = {1194--1198},
publisher = {ISCA},
title = {{SUPERB: Speech Processing Universal PERformance Benchmark}},
year = {2021}
}

@article{Giannakopoulos:2015,
author = {Giannakopoulos, Theodoros},
doi = {10.1371/journal.pone.0144610},
issn = {19326203},
journal = {PLoS One},
mendeley-groups = {ccc{\_}mse,ser{\_}nat,naturalness,Interspeech2020,cmmr2020,ref{\_}s3,ATSIP2020},
number = {12},
pages = {1--17},
title = {{pyAudioAnalysis: An open-source python library for audio signal analysis}},
volume = {10},
year = {2015}
}

@article{Watanabe:2018,
author = {Watanabe, Shinji and Hori, Takaaki and Karita, Shigeki and Hayashi, Tomoki and Nishitoba, Jiro and Unno, Yuya and Soplin, Nelson Enrique Yalta and Heymann, Jahn and Wiesner, Matthew and Chen, Nanxin and Renduchintala, Adithya and Ochiai, Tsubasa},
doi = {10.21437/Interspeech.2018-1456},
eprint = {1804.00015},
issn = {19909772},
journal = {Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH},
keywords = {Dynamical neural network,End-to-end,Kaldi,Open source software,Speech recognition},
number = {September},
pages = {2207--2211},
title = {{ESPNet: End-to-end speech processing toolkit}},
volume = {2018-Septe},
year = {2018}
}

@misc{speechbrain:2021,
  title={{SpeechBrain}: A General-Purpose Speech Toolkit},
  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and François Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},
  year={2021},
  eprint={2106.04624},
  archivePrefix={arXiv},
  primaryClass={eess.AS},
  note={arXiv:2106.04624}
}

@misc{spotlight:2023,
  author = {Suwelack, Stefan},
  title = {Spotlight},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Renumics/spotlight/}},
}

@article{opensmile:2010,
author = {Eyben, Florian and Wöllmer, Martin and Schuller, Björn},
year = {2010},
month = {01},
pages = {1459-1462},
title = {openSMILE -- The Munich Versatile and Fast Open-Source Audio Feature Extractor},
journal = {MM'10 - Proceedings of the ACM Multimedia 2010 International Conference},
doi = {10.1145/1873951.1874246}
}

@inproceedings{wav2vec:2020,
 author = {Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {12449--12460},
 publisher = {Curran Associates, Inc.},
 title = {wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations},
 url = {https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{burkhardt:2022-syntact,
address = {Marseille, France},
author = {Burkhardt, Felix and Eyben, Florian and Schuller, W},
booktitle = {Proc. Work. Dataset Creat. Low. Lang. within 13th Lang. Resour. Eval. Conf.},
editor = {{Jonne S{\"{a}}lev{\"{a}}} and Lignos, Constantine},
keywords = {database,emotional,simulation,speech synthesis,synthetic},
publisher = {European Language Resources Association},
title = {{SyntAct : A Synthesized Database of Basic Emotions}},
year = {2022}
}


@inproceedings{Atmaja:2024a,
abstract = {Speech classification has attracted increasing attention due to its wide applications, particularly in classifying physical and mental states. However, these tasks are challenging due to the high variability in speech signals. Ensemble learning has shown promising results when multiple classifiers are combined to improve performance. With recent advancements in hardware development, combining several models is not a limitation in deep learning research and applications. In this paper, we propose an uncertainty-based ensemble learning approach for speech classification. Specifically, we train a set of base features on the same classifier and quantify the uncertainty of their predictions. The predictions are combined using variants of uncertainty calculation to produce the final prediction. The visualization of the effect of uncertainty and its ensemble learning results show potential improvements in speech classification tasks. The proposed method outperforms single models and conventional ensemble learning methods in terms of unweighted accuracy or weighted accuracy.},
author = {Atmaja, Bagus Tris and Sasou, Akira and Burkhardt, Felix},
booktitle = {2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)},
pages = {1--6},
title = {Uncertainty-Based Ensemble Learning for Speech Classification},
year = {2024},
doi={10.1109/O-COCOSDA64382.2024.10800111}
}

@inproceedings{Burkhardt:2024,
author = {Burkhardt, Felix and Atmaja, Bagus Tris and Derington, Anna and Eyben, Florian},
booktitle = {2024 27th Conference of the Oriental COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA)},
pages = {1--6},
title = {Check Your Audio Data: Nkululeko for Bias Detection},
year = {2024},
doi={10.1109/O-COCOSDA64382.2024.10800580}
}

@inproceedings{Atmaja:2025,
author = {Atmaja, Bagus Tris and Sasou, Akira},
booktitle = {2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)},
title = {Pathological Voice Detection From Sustained Vowels: Handcrafted vs. Self-supervised Learning},
year = {2025}
}

@inproceedings{Atmaja:2025b,
abstract = {Speech classification has attracted increasing attention due to its wide applications, particularly in classifying physical and mental states. However, these tasks are challenging due to the high variability in speech signals. Ensemble learning has shown promising results when multiple classifiers are combined to improve performance. With recent advancements in hardware development, combining several models is not a limitation in deep learning research and applications. In this paper, we propose an uncertainty-based ensemble learning approach for speech classification. Specifically, we train a set of base features on the same classifier and quantify the uncertainty of their predictions. The predictions are combined using variants of uncertainty calculation to produce the final prediction. The visualization of the effect of uncertainty and its ensemble learning results show potential improvements in speech classification tasks. The proposed method outperforms single models and conventional ensemble learning methods in terms of unweighted accuracy or weighted accuracy.},
address = {Fukuoka},
author = {Atmaja, Bagus Tris and Burkhardt, Felix and Sasou, Akira},
booktitle = {2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)},
title = {{Performance-weighted Ensemble Learning for Speech Classification}},
year = {2025}
}

